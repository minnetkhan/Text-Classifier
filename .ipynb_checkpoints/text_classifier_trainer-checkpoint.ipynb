{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    importing all files...\n",
    "'''\n",
    "import pandas as pd\n",
    "import os, unicodedata, pickle\n",
    "import normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info about each class...\n",
      "                  class name  docs      prob\n",
      "0      talk.politics.mideast  1000  0.050008\n",
      "1                  rec.autos  1000  0.050008\n",
      "2      comp.sys.mac.hardware  1000  0.050008\n",
      "3                alt.atheism  1000  0.050008\n",
      "4         rec.sport.baseball  1000  0.050008\n",
      "5    comp.os.ms-windows.misc  1000  0.050008\n",
      "6           rec.sport.hockey  1000  0.050008\n",
      "7                  sci.crypt  1000  0.050008\n",
      "8                    sci.med  1000  0.050008\n",
      "9         talk.politics.misc  1000  0.050008\n",
      "10           rec.motorcycles  1000  0.050008\n",
      "11            comp.windows.x  1000  0.050008\n",
      "12             comp.graphics  1000  0.050008\n",
      "13  comp.sys.ibm.pc.hardware  1000  0.050008\n",
      "14           sci.electronics  1000  0.050008\n",
      "15        talk.politics.guns  1000  0.050008\n",
      "16                 sci.space  1000  0.050008\n",
      "17    soc.religion.christian   997  0.049857\n",
      "18              misc.forsale  1000  0.050008\n",
      "19        talk.religion.misc  1000  0.050008\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Training Model...\n",
    "'''\n",
    "\n",
    "#mapping index to class name[][0] and storing address of the class[][1]\n",
    "class_mapping_index = {}\n",
    "\n",
    "#keep count of docs in every class\n",
    "class_docs_count = []\n",
    "for i in range(0, 20):\n",
    "    class_docs_count.append(0)\n",
    "\n",
    "#count total docs\n",
    "total_docs = 0\n",
    "\n",
    "#probability of each class\n",
    "class_prob = {}\n",
    "\n",
    "'''\n",
    "    count words in each class\n",
    "    structure: list of dictionary\n",
    "    class_index: { word: frequency}\n",
    "'''\n",
    "count_each_word_frequency_in_each_class = []\n",
    "for i in range(0, 20):\n",
    "    count_each_word_frequency_in_each_class.append({})\n",
    "\n",
    "#total words in each class\n",
    "total_words_in_each_class = []\n",
    "for i in range(0, 20):\n",
    "    total_words_in_each_class.append(0)\n",
    "\n",
    "#total vocabulary\n",
    "total_vocabulary = 0\n",
    "\n",
    "#complete vocabulary set\n",
    "complete_vocabulary_set = set()\n",
    "\n",
    "#mapping class name with index\n",
    "for dirpath, dirnames, filenames in os.walk(\".././20_newsgroups\"):\n",
    "    for index, dirname in enumerate([d for d in dirnames]):\n",
    "        if not dirname.startswith(\".\"):\n",
    "            class_mapping_index[index] = [dirname, os.path.join(dirpath, dirname)]\n",
    "            \n",
    "#counting number of documents in each class\n",
    "for index in class_mapping_index:\n",
    "    for dirpath, dirnames, filenames in os.walk(class_mapping_index[index][1]):\n",
    "        for filename in filenames:\n",
    "            if not filename.startswith(\".\"):\n",
    "                class_docs_count[index] += 1    \n",
    "#counting total number of documents\n",
    "for index, document_in_each_class in enumerate(class_docs_count):\n",
    "    total_docs += document_in_each_class\n",
    "\n",
    "#calculating each class probability\n",
    "print(\"Info about each class...\")\n",
    "header = [\"class name\", \"docs\", \"prob\"]\n",
    "data = {}\n",
    "for index, docs_in_each_class in enumerate(class_docs_count):\n",
    "    class_prob[index] = (docs_in_each_class/total_docs)\n",
    "    data[index] = [class_mapping_index[index][0] , class_docs_count[index], class_prob[index]]\n",
    "\n",
    "\n",
    "df = pd.DataFrame.from_dict(data, orient='index', columns=header)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total words in each class: \n",
      "0 talk.politics.mideast 343895\n",
      "1 rec.autos 174451\n",
      "2 comp.sys.mac.hardware 154821\n",
      "3 alt.atheism 235726\n",
      "4 rec.sport.baseball 186909\n",
      "5 comp.os.ms-windows.misc 399957\n",
      "6 rec.sport.hockey 233302\n",
      "7 sci.crypt 248386\n",
      "8 sci.med 225155\n",
      "9 talk.politics.misc 291295\n",
      "10 rec.motorcycles 164886\n",
      "11 comp.windows.x 243245\n",
      "12 comp.graphics 209390\n",
      "13 comp.sys.ibm.pc.hardware 165204\n",
      "14 sci.electronics 167339\n",
      "15 talk.politics.guns 250252\n",
      "16 sci.space 227040\n",
      "17 soc.religion.christian 253878\n",
      "18 misc.forsale 142989\n",
      "19 talk.religion.misc 243992\n",
      "total vocabulary of corpus:  261306\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    opening each and every documents class wise\n",
    "    preforming normalization in each document\n",
    "    storing words present in each class with its frequency\n",
    "    and storing vocabulary of entire corpus\n",
    "'''\n",
    "\n",
    "for class_index in class_mapping_index:\n",
    "    for dirpath, dirnames, filenames in os.walk(class_mapping_index[class_index][1]):\n",
    "        for filename in [f for f in filenames]:\n",
    "            if not filename.startswith(\".\"):\n",
    "                file1 = open(os.path.join(dirpath, filename), 'r', errors='ignore')\n",
    "                file = file1.read();\n",
    "\n",
    "                #text preprosessing and normalization\n",
    "                file = normalization.normalize(file)\n",
    "\n",
    "\n",
    "                for word in file:\n",
    "                    complete_vocabulary_set.add(word)\n",
    "\n",
    "                    if word in count_each_word_frequency_in_each_class[class_index]:\n",
    "                        count_each_word_frequency_in_each_class[class_index][word] += 1\n",
    "                    else:\n",
    "                        count_each_word_frequency_in_each_class[class_index].update({word: 1})\n",
    "\n",
    "                #closing opened file\n",
    "                file1.close()\n",
    "\n",
    "        for word, frequency in count_each_word_frequency_in_each_class[class_index].items():\n",
    "            total_words_in_each_class[class_index] += frequency\n",
    "\n",
    "total_vocabulary = len(complete_vocabulary_set)\n",
    "\n",
    "\n",
    "print(\"total words in each class: \")\n",
    "for index in class_mapping_index:\n",
    "    print(index, class_mapping_index[index][0] , total_words_in_each_class[index])\n",
    "\n",
    "print(\"total vocabulary of corpus: \", total_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = open(\"./training/pickled_model\", \"wb\")\n",
    "model = [class_mapping_index, class_docs_count, total_docs, class_prob, \n",
    "               count_each_word_frequency_in_each_class, total_words_in_each_class, total_vocabulary]\n",
    "\n",
    "pickle.dump(model, pickle_file)\n",
    "pickle_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
